{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dfe56f1-d591-4d7f-b6c1-f8818406b35a",
   "metadata": {},
   "source": [
    "### QUESTION 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d0459-7a1a-4183-905a-6b9f3bf8dfd0",
   "metadata": {},
   "source": [
    "#### What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4629f2-991b-4347-b58f-773c7c291623",
   "metadata": {},
   "source": [
    "##### Min-Max scaling, also known as normalization, is a data preprocessing technique that rescales numeric data to a common range, usually between 0 and 1, to prevent features with large ranges from dominating the model. It's calculated as:\n",
    "\n",
    "##### X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "##### Where:\n",
    "\n",
    "##### - X is the original value\n",
    "##### - X_min is the minimum value in the dataset\n",
    "##### - X_max is the maximum value in the dataset\n",
    "\n",
    "##### Example:\n",
    "\n",
    "##### Suppose we have a dataset with two features: Age and Income.\n",
    "\n",
    "##### | Age | Income |\n",
    "##### | --- | --- |\n",
    "##### | 25  | 50000 |\n",
    "##### | 30  | 60000 |\n",
    "##### | 35  | 70000 |\n",
    "\n",
    "##### After Min-Max scaling:\n",
    "\n",
    "##### | Age_scaled | Income_scaled |\n",
    "##### | --- | --- |\n",
    "##### | 0.0 | 0.0 |\n",
    "##### | 0.5 | 0.5 |\n",
    "##### | 1.0 | 1.0 |\n",
    "\n",
    "##### In this example, the Age and Income features are rescaled to the range [0, 1], ensuring that both features contribute equally to the model.\n",
    "\n",
    "##### Min-Max scaling is useful when:\n",
    "\n",
    "##### - Features have different units or scales.\n",
    "##### - Features have significantly different ranges.\n",
    "##### - Algorithms assume normalized data (e.g., neural networks, SVMs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e47e5-5879-4562-a40a-9d524914a9d3",
   "metadata": {},
   "source": [
    "### QUESTION 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73931115-3f2c-4102-8d58-6ab55224a262",
   "metadata": {},
   "source": [
    "##### The Unit Vector technique, also known as L2 normalization or Euclidean normalization, scales features by dividing each value by the Euclidean norm (magnitude) of the vector. This ensures that each feature vector has a length of 1, making all features equally important.\n",
    "\n",
    "##### Unit Vector scaling:\n",
    "\n",
    "##### X_scaled = X / ||X||\n",
    "\n",
    "##### Where:\n",
    "\n",
    "##### - X is the original feature vector\n",
    "##### - ||X|| is the Euclidean norm (magnitude) of X\n",
    "\n",
    "##### Example:\n",
    "\n",
    "##### Suppose we have a dataset with two features: Age and Income.\n",
    "\n",
    "##### | Age | Income |\n",
    "##### | --- | --- |\n",
    "##### | 25  | 50000 |\n",
    "##### | 30  | 60000 |\n",
    "##### | 35  | 70000 |\n",
    "\n",
    "##### First, calculate the Euclidean norm for each row:\n",
    "\n",
    "##### | Age | Income | Norm |\n",
    "##### | --- | --- | --- |\n",
    "##### | 25  | 50000 | 50025.01 |\n",
    "##### | 30  | 60000 | 60030.01 |\n",
    "##### | 35  | 70000 | 70035.01 |\n",
    "\n",
    "##### Then, scale each feature by the norm:\n",
    "\n",
    "##### | Age_scaled | Income_scaled |\n",
    "##### | --- | --- |\n",
    "##### | 25/50025.01 | 50000/50025.01 |\n",
    "##### | 30/60030.01 | 60000/60030.01 |\n",
    "##### | 35/70035.01 | 70000/70035.01 |\n",
    "\n",
    "##### Unit Vector scaling differs from Min-Max scaling in that:\n",
    "\n",
    "##### - Min-Max scaling rescales to a fixed range (usually [0, 1]), while Unit Vector scaling rescales to a fixed length (1).\n",
    "##### - Min-Max scaling is sensitive to outliers, while Unit Vector scaling is more robust.\n",
    "\n",
    "##### Unit Vector scaling is useful when:\n",
    "\n",
    "##### - Features have different units or scales.\n",
    "##### - Features should be treated as directions rather than magnitudes.\n",
    "##### - Algorithms assume normalized data (e.g., cosine similarity, neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20214fa-7a43-4c41-b098-e667ce5d24d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc4ba380-2d9e-4a7c-be08-1cd2ef9ff3ae",
   "metadata": {},
   "source": [
    "##### Q3. PCA (Principal Component Analysis) is a dimensionality reduction technique that transforms high-dimensional data into lower-dimensional data while retaining most of the information. It's used to identify patterns and correlations in data.\n",
    "\n",
    "##### Example: A dataset with features [height, weight, age] can be reduced to two principal components (PCs) using PCA, capturing most of the variance.\n",
    "\n",
    "##### Q4. PCA is a Feature Extraction technique that transforms original features into new, uncorrelated features (principal components) that capture most of the data's variance.\n",
    "\n",
    "##### Example: In image processing, PCA can be used to extract features from images, reducing dimensionality while retaining important information.\n",
    "\n",
    "##### Q5. To preprocess the data using Min-Max scaling:\n",
    "\n",
    "##### 1. Calculate the minimum and maximum values for each feature (price, rating, delivery time).\n",
    "##### 2. Apply the Min-Max scaling formula to rescale each feature to a common range (e.g., [0, 1]).\n",
    "\n",
    "##### Q6. To reduce dimensionality using PCA:\n",
    "\n",
    "##### 1. Standardize the data (mean=0, variance=1).\n",
    "##### 2. Calculate the covariance matrix.\n",
    "##### 3. Compute eigenvectors and eigenvalues.\n",
    "##### 4. Select the top k eigenvectors (principal components) that capture most of the variance.\n",
    "##### 5. Project the original data onto the selected principal components.\n",
    "\n",
    "##### Q7. To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] to transform the values to a range of -1 to 1:\n",
    "\n",
    "##### 1. Calculate the minimum and maximum values: min=1, max=20.\n",
    "##### 2. Apply the Min-Max scaling formula: (x - min) / (max - min) * 2 - 1.\n",
    "##### 3. Rescaled values: [-1, -0.6, -0.2, 0.2, 1].\n",
    "\n",
    "##### Q8. To perform Feature Extraction using PCA on the dataset [height, weight, age, gender, blood pressure]:\n",
    "\n",
    "##### 1. Standardize the data.\n",
    "##### 2. Compute the covariance matrix.\n",
    "##### 3. Calculate eigenvectors and eigenvalues.\n",
    "##### 4. Select the top k eigenvectors (e.g., 2-3) that capture most of the variance.\n",
    "##### 5. Retain the selected principal components as new features.\n",
    "\n",
    "##### Note: The number of principal components to retain depends on the dataset and desired level of dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b7852-6dbc-4033-b971-1bba6363bec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
