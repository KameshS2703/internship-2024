{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb604dec-503e-49fe-aef4-4aa87f8d825d",
   "metadata": {},
   "source": [
    "### Q1: Concept of R-squared\n",
    "\n",
    "**R-squared** (Coefficient of Determination) measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "**Calculation**:\n",
    "- **Formula**: \n",
    "  \\[\n",
    "  R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}}\n",
    "  \\]\n",
    "  where:\n",
    "  - \\(\\text{SS}_{\\text{res}}\\) is the sum of squared residuals (errors).\n",
    "  - \\(\\text{SS}_{\\text{tot}}\\) is the total sum of squares (variance of the dependent variable).\n",
    "\n",
    "**Representation**: \n",
    "- **Range**: \\(0 \\leq R^2 \\leq 1\\).\n",
    "- **Interpretation**: An \\(R^2\\) of 0 means the model explains none of the variance, while an \\(R^2\\) of 1 means the model explains all the variance.\n",
    "\n",
    "### Q2: Adjusted R-squared\n",
    "\n",
    "**Adjusted R-squared** adjusts the R-squared value for the number of predictors in the model. It accounts for the number of predictors and is particularly useful for comparing models with different numbers of predictors.\n",
    "\n",
    "**Calculation**:\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  \\text{Adjusted } R^2 = 1 - \\left( \\frac{1 - R^2}{n - p - 1} \\right) \\times (n - 1)\n",
    "  \\]\n",
    "  where:\n",
    "  - \\(n\\) = number of observations\n",
    "  - \\(p\\) = number of predictors\n",
    "\n",
    "**Difference from R-squared**:\n",
    "- **R-squared** can increase with more predictors, even if they do not improve the model.\n",
    "- **Adjusted R-squared** can decrease if the added predictors do not improve the modelâ€™s performance.\n",
    "\n",
    "### Q3: When to Use Adjusted R-squared\n",
    "\n",
    "**Adjusted R-squared** is more appropriate when:\n",
    "- **Comparing Models**: Evaluating models with different numbers of predictors.\n",
    "- **Model Selection**: Ensuring that adding more predictors does not merely increase the R-squared value but also improves the model's performance.\n",
    "\n",
    "### Q4: RMSE, MSE, and MAE in Regression Analysis\n",
    "\n",
    "**RMSE (Root Mean Squared Error)**:\n",
    "- **Definition**: The square root of the average of the squared differences between predicted and actual values.\n",
    "- **Calculation**:\n",
    "  \\[\n",
    "  \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "  \\]\n",
    "- **Representation**: Provides the average magnitude of error in the units of the dependent variable.\n",
    "\n",
    "**MSE (Mean Squared Error)**:\n",
    "- **Definition**: The average of the squared differences between predicted and actual values.\n",
    "- **Calculation**:\n",
    "  \\[\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "  \\]\n",
    "- **Representation**: Emphasizes larger errors due to squaring.\n",
    "\n",
    "**MAE (Mean Absolute Error)**:\n",
    "- **Definition**: The average of the absolute differences between predicted and actual values.\n",
    "- **Calculation**:\n",
    "  \\[\n",
    "  \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "  \\]\n",
    "- **Representation**: Provides the average magnitude of errors without squaring.\n",
    "\n",
    "### Q5: Advantages and Disadvantages of RMSE, MSE, and MAE\n",
    "\n",
    "**Advantages**:\n",
    "- **RMSE**: Penalizes larger errors more heavily; useful when large errors are undesirable.\n",
    "- **MSE**: Provides a measure of the average error; mathematically tractable.\n",
    "- **MAE**: Simple to interpret and less sensitive to outliers.\n",
    "\n",
    "**Disadvantages**:\n",
    "- **RMSE**: Sensitive to outliers; can be misleading if the data has large errors.\n",
    "- **MSE**: Can be hard to interpret due to squaring; sensitive to outliers.\n",
    "- **MAE**: Does not capture the variance of errors; less sensitive to large errors.\n",
    "\n",
    "### Q6: Lasso Regularization\n",
    "\n",
    "**Lasso (Least Absolute Shrinkage and Selection Operator)**:\n",
    "- **Definition**: A regularization technique that adds a penalty proportional to the absolute value of the coefficients.\n",
    "- **Penalty Term**:\n",
    "  \\[\n",
    "  \\text{Cost Function} = \\text{RSS} + \\lambda \\sum_{j=1}^p |\\beta_j|\n",
    "  \\]\n",
    "  where \\( \\lambda \\) is the regularization parameter.\n",
    "\n",
    "**Difference from Ridge Regularization**:\n",
    "- **Lasso**: Can reduce some coefficients to zero, effectively performing feature selection.\n",
    "- **Ridge**: Penalizes the sum of squared coefficients, but does not typically reduce coefficients to zero.\n",
    "\n",
    "**When to Use**:\n",
    "- Use **Lasso** when feature selection is desired or when you have many predictors and suspect some are not useful.\n",
    "\n",
    "### Q7: Preventing Overfitting with Regularized Linear Models\n",
    "\n",
    "**How It Helps**:\n",
    "- **Regularization**: Adds a penalty for larger coefficients, which helps to constrain the model and prevent it from fitting noise in the training data.\n",
    "\n",
    "**Example**:\n",
    "- A dataset with many features might lead to overfitting with a standard linear regression model. Using Ridge or Lasso regularization can reduce the impact of less significant features and improve generalization.\n",
    "\n",
    "### Q8: Limitations of Regularized Linear Models\n",
    "\n",
    "**Limitations**:\n",
    "- **Model Complexity**: Regularization methods add complexity and may not always handle highly non-linear relationships well.\n",
    "- **Choice of Regularization Parameter**: Selecting an appropriate value for the regularization parameter requires cross-validation and may not always be straightforward.\n",
    "\n",
    "### Q9: Comparing Models with RMSE and MAE\n",
    "\n",
    "**Model A**: RMSE = 10\n",
    "**Model B**: MAE = 8\n",
    "\n",
    "**Choice of Model**:\n",
    "- **MAE**: Preferred when you want a metric that is less sensitive to outliers.\n",
    "- **RMSE**: Preferred if larger errors are more critical to your analysis.\n",
    "\n",
    "**Limitations**:\n",
    "- **MAE**: Does not penalize large errors, which may be important depending on the context.\n",
    "- **RMSE**: Can be influenced heavily by outliers.\n",
    "\n",
    "### Q10: Comparing Regularized Models with Ridge and Lasso\n",
    "\n",
    "**Model A**: Ridge Regularization (\\(\\lambda = 0.1\\))\n",
    "**Model B**: Lasso Regularization (\\(\\lambda = 0.5\\))\n",
    "\n",
    "**Choice of Model**:\n",
    "- **Ridge**: Use if you want to shrink coefficients but retain all features.\n",
    "- **Lasso**: Use if you want to perform feature selection and reduce some coefficients to zero.\n",
    "\n",
    "**Trade-offs**:\n",
    "- **Ridge**: Tends to include all features but with smaller coefficients.\n",
    "- **Lasso**: Can lead to sparser models with some coefficients set to zero, which may be useful for feature selection but can potentially miss some subtle relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51fcec4-9a8e-4bef-8bfe-8082811f4c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
