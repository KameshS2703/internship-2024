{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba846950-ef8a-4cf2-a239-365cd6936dac",
   "metadata": {},
   "source": [
    "### Q1: Mathematical Formula for a Linear SVM\n",
    "\n",
    "In a linear Support Vector Machine (SVM), the goal is to find the optimal hyperplane that separates the classes with the maximum margin. \n",
    "\n",
    "The decision function of a linear SVM is given by:\n",
    "\n",
    "\\[ f(x) = \\mathbf{w}^T \\mathbf{x} + b \\]\n",
    "\n",
    "where:\n",
    "- \\(\\mathbf{w}\\) is the weight vector.\n",
    "- \\(\\mathbf{x}\\) is the input feature vector.\n",
    "- \\(b\\) is the bias term.\n",
    "\n",
    "The hyperplane is defined by the equation:\n",
    "\n",
    "\\[ \\mathbf{w}^T \\mathbf{x} + b = 0 \\]\n",
    "\n",
    "### Q2: Objective Function of a Linear SVM\n",
    "\n",
    "The objective of a linear SVM is to maximize the margin between two classes while minimizing classification errors. This can be formulated as a convex optimization problem:\n",
    "\n",
    "\\[ \\text{Minimize} \\quad \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\]\n",
    "\n",
    "Subject to:\n",
    "\n",
    "\\[ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 \\quad \\text{for all } i \\]\n",
    "\n",
    "where \\(y_i\\) is the class label for sample \\(\\mathbf{x}_i\\). This formulation ensures that the margin is maximized.\n",
    "\n",
    "### Q3: Kernel Trick in SVM\n",
    "\n",
    "The kernel trick is a method used to extend SVMs to handle non-linearly separable data by mapping input features into a higher-dimensional space. \n",
    "\n",
    "For a non-linear problem, the decision function is:\n",
    "\n",
    "\\[ f(x) = \\sum_{i=1}^N \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b \\]\n",
    "\n",
    "where \\(K(\\mathbf{x}_i, \\mathbf{x})\\) is a kernel function, such as the polynomial or radial basis function (RBF) kernel. The kernel function computes the dot product in a higher-dimensional space without explicitly performing the mapping.\n",
    "\n",
    "### Q4: Role of Support Vectors in SVM\n",
    "\n",
    "Support vectors are the data points that are closest to the hyperplane and influence its position and orientation. They are critical in determining the optimal hyperplane. \n",
    "\n",
    "**Example:**\n",
    "- If we have two classes of data points, the support vectors are the points lying on the margin boundaries (i.e., the points closest to the hyperplane).\n",
    "- These support vectors are used to maximize the margin and are the only data points that affect the position of the hyperplane.\n",
    "\n",
    "### Q5: Illustrations of SVM Components\n",
    "\n",
    "**Hyperplane:** The decision boundary that separates the classes.\n",
    "\n",
    "**Margin:** The distance between the hyperplane and the closest data points from each class. The width of the margin is maximized in SVM.\n",
    "\n",
    "**Soft Margin:** Allows some data points to be within the margin or even misclassified to handle non-linearly separable cases or noisy data. Controlled by the regularization parameter \\(C\\).\n",
    "\n",
    "**Hard Margin:** Assumes the data is linearly separable with no errors. This is a strict case where no points are allowed inside the margin or misclassified.\n",
    "\n",
    "**Graphs:**\n",
    "\n",
    "1. **Hyperplane and Margin:**\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   import matplotlib.pyplot as plt\n",
    "   from sklearn import datasets\n",
    "   from sklearn.svm import SVC\n",
    "   from sklearn.preprocessing import StandardScaler\n",
    "   from sklearn.model_selection import train_test_split\n",
    "\n",
    "   # Load and preprocess the data\n",
    "   iris = datasets.load_iris()\n",
    "   X = iris.data[:, :2]  # Only use the first two features for easy visualization\n",
    "   y = iris.target\n",
    "   X = X[y != 2]  # Binary classification (Setosa vs. Non-Setosa)\n",
    "   y = y[y != 2]\n",
    "\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "   scaler = StandardScaler()\n",
    "   X_train = scaler.fit_transform(X_train)\n",
    "   X_test = scaler.transform(X_test)\n",
    "\n",
    "   # Train the SVM model\n",
    "   clf = SVC(kernel='linear', C=1)\n",
    "   clf.fit(X_train, y_train)\n",
    "\n",
    "   # Plot decision boundary\n",
    "   h = .02  # step size in the mesh\n",
    "   x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "   y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "   xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "   Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "   Z = Z.reshape(xx.shape)\n",
    "\n",
    "   plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "   plt.scatter(X_train[:, 0], X_train[:, 1], c=y, edgecolors='k', marker='o')\n",
    "   plt.title('SVM Decision Boundary with Hard Margin')\n",
    "   plt.xlabel('Feature 1')\n",
    "   plt.ylabel('Feature 2')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "2. **Soft Margin vs. Hard Margin:**\n",
    "   ```python\n",
    "   # Plotting for Soft Margin\n",
    "   clf_soft = SVC(kernel='linear', C=0.1)  # Lower C for soft margin\n",
    "   clf_soft.fit(X_train, y_train)\n",
    "\n",
    "   Z_soft = clf_soft.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "   Z_soft = Z_soft.reshape(xx.shape)\n",
    "\n",
    "   plt.contourf(xx, yy, Z_soft, alpha=0.8)\n",
    "   plt.scatter(X_train[:, 0], X_train[:, 1], c=y, edgecolors='k', marker='o')\n",
    "   plt.title('SVM Decision Boundary with Soft Margin (C=0.1)')\n",
    "   plt.xlabel('Feature 1')\n",
    "   plt.ylabel('Feature 2')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "### Q6: SVM Implementation with the Iris Dataset\n",
    "\n",
    "1. **Load and Split Data**:\n",
    "   ```python\n",
    "   from sklearn.datasets import load_iris\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.svm import SVC\n",
    "   from sklearn.metrics import accuracy_score\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   # Load Iris dataset\n",
    "   iris = load_iris()\n",
    "   X = iris.data\n",
    "   y = iris.target\n",
    "\n",
    "   # For binary classification, we will use only two classes\n",
    "   X = X[y != 2]\n",
    "   y = y[y != 2]\n",
    "\n",
    "   # Split data\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "   # Train SVM model\n",
    "   clf = SVC(kernel='linear', C=1.0)\n",
    "   clf.fit(X_train, y_train)\n",
    "\n",
    "   # Predict and evaluate\n",
    "   y_pred = clf.predict(X_test)\n",
    "   accuracy = accuracy_score(y_test, y_pred)\n",
    "   print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "   # Plot decision boundaries\n",
    "   X2D = X[:, :2]  # Use only the first two features\n",
    "   X_train2D, X_test2D, y_train2D, y_test2D = train_test_split(X2D, y, test_size=0.3, random_state=42)\n",
    "   clf2D = SVC(kernel='linear', C=1.0)\n",
    "   clf2D.fit(X_train2D, y_train2D)\n",
    "\n",
    "   h = .02  # Step size\n",
    "   x_min, x_max = X2D[:, 0].min() - 1, X2D[:, 0].max() + 1\n",
    "   y_min, y_max = X2D[:, 1].min() - 1, X2D[:, 1].max() + 1\n",
    "   xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "   Z = clf2D.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "   Z = Z.reshape(xx.shape)\n",
    "\n",
    "   plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "   plt.scatter(X_train2D[:, 0], X_train2D[:, 1], c=y_train2D, edgecolors='k', marker='o')\n",
    "   plt.title('SVM Decision Boundary with Linear Kernel')\n",
    "   plt.xlabel('Feature 1')\n",
    "   plt.ylabel('Feature 2')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "2. **Varying Regularization Parameter \\(C\\)**:\n",
    "   ```python\n",
    "   C_values = [0.01, 0.1, 1, 10, 100]\n",
    "   for C in C_values:\n",
    "       clf = SVC(kernel='linear', C=C)\n",
    "       clf.fit(X_train, y_train)\n",
    "       y_pred = clf.predict(X_test)\n",
    "       accuracy = accuracy_score(y_test, y_pred)\n",
    "       print(f'C={C} - Accuracy: {accuracy:.2f}')\n",
    "   ```\n",
    "\n",
    "### Bonus Task: Implementing Linear SVM from Scratch\n",
    "\n",
    "1. **Implement Linear SVM**:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   class LinearSVM:\n",
    "       def __init__(self, C=1.0, learning_rate=0.001, num_iterations=1000):\n",
    "           self.C = C\n",
    "           self.learning_rate = learning_rate\n",
    "           self.num_iterations = num_iterations\n",
    "\n",
    "       def fit(self, X, y):\n",
    "           self.W = np.zeros(X.shape[1])\n",
    "           self.b = 0\n",
    "           m = X.shape[0]\n",
    "\n",
    "           for _ in range(self.num_iterations):\n",
    "               for i in range(m):\n",
    "                   if y[i] * (np.dot\n",
    "\n",
    "(X[i], self.W) + self.b) < 1:\n",
    "                       self.W -= self.learning_rate * (2 * self.W - self.C * y[i] * X[i])\n",
    "                       self.b -= self.learning_rate * self.C * y[i]\n",
    "                   else:\n",
    "                       self.W -= self.learning_rate * 2 * self.W\n",
    "\n",
    "       def predict(self, X):\n",
    "           return np.sign(np.dot(X, self.W) + self.b)\n",
    "\n",
    "   # Implement and compare with scikit-learn\n",
    "   from sklearn.metrics import accuracy_score\n",
    "\n",
    "   svm_scratch = LinearSVM(C=1.0)\n",
    "   svm_scratch.fit(X_train, y_train)\n",
    "   y_pred_scratch = svm_scratch.predict(X_test)\n",
    "   accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "   print(f'Scratch SVM Accuracy: {accuracy_scratch:.2f}')\n",
    "\n",
    "   # Compare with scikit-learn SVM\n",
    "   clf_sklearn = SVC(kernel='linear', C=1.0)\n",
    "   clf_sklearn.fit(X_train, y_train)\n",
    "   y_pred_sklearn = clf_sklearn.predict(X_test)\n",
    "   accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "   print(f'Scikit-Learn SVM Accuracy: {accuracy_sklearn:.2f}')\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9765ae-8c9b-492f-8e47-08f326119701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
