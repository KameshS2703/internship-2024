{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414bcfa6-a043-4b25-ba83-66f27363cb93",
   "metadata": {},
   "source": [
    "### Q1: Relationship Between Polynomial Functions and Kernel Functions\n",
    "\n",
    "Polynomial functions and kernel functions are both used to extend the capabilities of machine learning algorithms, particularly Support Vector Machines (SVMs).\n",
    "\n",
    "- **Polynomial Functions**: These are mathematical functions where the relationship between variables is expressed as a polynomial equation. For example, a polynomial function of degree \\(d\\) in two variables \\(x\\) and \\(y\\) is given by:\n",
    "  \n",
    "  \\[ f(x, y) = a_0 + a_1 x + a_2 y + a_3 x^2 + a_4 xy + a_5 y^2 + \\cdots + a_d x^d \\]\n",
    "\n",
    "- **Kernel Functions**: In machine learning, kernel functions are used to transform data into a higher-dimensional space without explicitly performing the transformation. This allows algorithms like SVMs to find a hyperplane that can separate data that is not linearly separable in the original space.\n",
    "\n",
    "  The polynomial kernel function is a specific type of kernel function given by:\n",
    "\n",
    "  \\[ K(x, x') = (x \\cdot x' + c)^d \\]\n",
    "\n",
    "  where \\(x\\) and \\(x'\\) are input vectors, \\(c\\) is a constant, and \\(d\\) is the degree of the polynomial. The polynomial kernel implicitly maps the input features into a higher-dimensional polynomial space.\n",
    "\n",
    "  **Relationship**: The polynomial kernel function allows SVMs to learn polynomial decision boundaries by implicitly mapping the input features to a higher-dimensional space, making it possible to model more complex relationships between the features.\n",
    "\n",
    "### Q2: Implementing SVM with a Polynomial Kernel in Python using Scikit-learn\n",
    "\n",
    "Here's how to implement an SVM with a polynomial kernel using Scikit-learn:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# For simplicity, we'll use only the first two features and convert it to a binary classification problem\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train SVM with polynomial kernel\n",
    "clf = SVC(kernel='poly', degree=3, C=1.0, coef0=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Plot decision boundary (using first two features for visualization)\n",
    "h = .02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', marker='o')\n",
    "plt.title('SVM with Polynomial Kernel')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Q3: Effect of Epsilon on Support Vectors in SVR\n",
    "\n",
    "In Support Vector Regression (SVR), the epsilon parameter (\\(\\epsilon\\)) defines the margin of tolerance where no penalty is given for errors. Increasing \\(\\epsilon\\) allows the model to be less sensitive to small errors, which can lead to fewer support vectors because the model becomes more tolerant of deviations.\n",
    "\n",
    "**Example**: With a higher \\(\\epsilon\\), more data points may fall within the margin of tolerance, resulting in fewer points being considered support vectors. Conversely, a lower \\(\\epsilon\\) requires the model to be more precise, potentially increasing the number of support vectors.\n",
    "\n",
    "### Q4: Effect of Hyperparameters on SVR Performance\n",
    "\n",
    "1. **Kernel Function**: The choice of kernel function (linear, polynomial, RBF) affects the decision boundary. For non-linear relationships, the polynomial or RBF kernel can capture more complex patterns. The linear kernel is suitable for linearly separable data.\n",
    "\n",
    "2. **C Parameter**: The regularization parameter \\(C\\) controls the trade-off between achieving a low error on the training data and minimizing the model complexity. A higher \\(C\\) value aims for a lower training error (more risk of overfitting), while a lower \\(C\\) encourages a simpler model (more risk of underfitting).\n",
    "\n",
    "3. **Epsilon Parameter**: The \\(\\epsilon\\) parameter specifies the margin of tolerance within which no penalty is given. A larger \\(\\epsilon\\) results in fewer support vectors and a simpler model, while a smaller \\(\\epsilon\\) provides a more precise model but may increase the number of support vectors.\n",
    "\n",
    "4. **Gamma Parameter**: In the RBF kernel, the \\(\\gamma\\) parameter defines the influence of a single training example. A high \\(\\gamma\\) value makes the model more sensitive to the data points, leading to a complex model, while a low \\(\\gamma\\) value creates a smoother decision boundary.\n",
    "\n",
    "### Q5: Assignment\n",
    "\n",
    "**Steps to follow:**\n",
    "\n",
    "1. **Import Libraries and Load Dataset**:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   import pandas as pd\n",
    "   from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "   from sklearn.preprocessing import StandardScaler\n",
    "   from sklearn.svm import SVC\n",
    "   from sklearn.metrics import accuracy_score, classification_report\n",
    "   import joblib  # For saving the model\n",
    "\n",
    "   # Load dataset\n",
    "   url = 'path/to/your/dataset.csv'\n",
    "   df = pd.read_csv(url)\n",
    "\n",
    "   # Separate features and target variable\n",
    "   X = df.drop('target', axis=1)\n",
    "   y = df['target']\n",
    "   ```\n",
    "\n",
    "2. **Split Dataset**:\n",
    "   ```python\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "   ```\n",
    "\n",
    "3. **Preprocess Data**:\n",
    "   ```python\n",
    "   scaler = StandardScaler()\n",
    "   X_train = scaler.fit_transform(X_train)\n",
    "   X_test = scaler.transform(X_test)\n",
    "   ```\n",
    "\n",
    "4. **Train SVC Classifier**:\n",
    "   ```python\n",
    "   clf = SVC()\n",
    "   clf.fit(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "5. **Predict and Evaluate**:\n",
    "   ```python\n",
    "   y_pred = clf.predict(X_test)\n",
    "   accuracy = accuracy_score(y_test, y_pred)\n",
    "   print(f'Accuracy: {accuracy:.2f}')\n",
    "   print(classification_report(y_test, y_pred))\n",
    "   ```\n",
    "\n",
    "6. **Tune Hyperparameters**:\n",
    "   ```python\n",
    "   param_grid = {\n",
    "       'C': [0.1, 1, 10],\n",
    "       'gamma': [0.01, 0.1, 1],\n",
    "       'kernel': ['linear', 'poly', 'rbf']\n",
    "   }\n",
    "   grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
    "   grid_search.fit(X_train, y_train)\n",
    "   best_clf = grid_search.best_estimator_\n",
    "   print(f'Best Parameters: {grid_search.best_params_}')\n",
    "   ```\n",
    "\n",
    "7. **Save the Trained Model**:\n",
    "   ```python\n",
    "   joblib.dump(best_clf, 'svm_model.pkl')\n",
    "   ```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Polynomial Kernel**: Extends SVMs to handle non-linear relationships.\n",
    "- **SVR Parameters**: \\(\\epsilon\\), \\(C\\), \\(\\gamma\\) affect the modelâ€™s sensitivity, complexity, and generalization.\n",
    "- **Implementation**: Involves training, evaluating, and tuning an SVM model with hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877748f-c223-430a-beec-983efcc3c640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
