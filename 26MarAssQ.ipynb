{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7a1f94-efc2-4255-9416-debf5540ca0a",
   "metadata": {},
   "source": [
    "### Q1: Difference Between Simple Linear Regression and Multiple Linear Regression\n",
    "\n",
    "**Simple Linear Regression**:\n",
    "- **Definition**: It models the relationship between two variables: one independent variable \\( X \\) and one dependent variable \\( Y \\). The relationship is represented by a straight line.\n",
    "- **Equation**: \\( Y = \\beta_0 + \\beta_1 X + \\epsilon \\)\n",
    "  - \\( \\beta_0 \\) = intercept\n",
    "  - \\( \\beta_1 \\) = slope\n",
    "  - \\( \\epsilon \\) = error term\n",
    "\n",
    "**Example**: Predicting a personâ€™s weight (\\( Y \\)) based on their height (\\( X \\)).\n",
    "\n",
    "**Multiple Linear Regression**:\n",
    "- **Definition**: It models the relationship between one dependent variable \\( Y \\) and multiple independent variables \\( X_1, X_2, ..., X_n \\). The relationship is represented by a hyperplane.\n",
    "- **Equation**: \\( Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon \\)\n",
    "  - \\( \\beta_0 \\) = intercept\n",
    "  - \\( \\beta_1, \\beta_2, ..., \\beta_n \\) = coefficients for each predictor\n",
    "  - \\( \\epsilon \\) = error term\n",
    "\n",
    "**Example**: Predicting a person's weight (\\( Y \\)) based on their height (\\( X_1 \\)), age (\\( X_2 \\)), and gender (\\( X_3 \\)).\n",
    "\n",
    "### Q2: Assumptions of Linear Regression\n",
    "\n",
    "1. **Linearity**: The relationship between the independent and dependent variables is linear.\n",
    "   - **Check**: Scatter plots of residuals vs. predicted values should show no patterns.\n",
    "2. **Independence**: Observations are independent of each other.\n",
    "   - **Check**: Durbin-Watson test for autocorrelation in residuals.\n",
    "3. **Homoscedasticity**: The residuals have constant variance.\n",
    "   - **Check**: Residuals vs. fitted values plot should show a random scatter.\n",
    "4. **Normality of Residuals**: Residuals are normally distributed.\n",
    "   - **Check**: Q-Q plot or histogram of residuals.\n",
    "5. **No Multicollinearity**: Independent variables are not too highly correlated.\n",
    "   - **Check**: Variance Inflation Factor (VIF) values.\n",
    "\n",
    "### Q3: Interpreting the Slope and Intercept in Linear Regression\n",
    "\n",
    "- **Intercept (\\( \\beta_0 \\))**: The expected value of \\( Y \\) when \\( X \\) is 0.\n",
    "- **Slope (\\( \\beta_1 \\))**: The change in \\( Y \\) for a one-unit change in \\( X \\).\n",
    "\n",
    "**Example**: In a model predicting salary based on years of experience:\n",
    "- **Intercept**: Represents the estimated starting salary for someone with zero years of experience.\n",
    "- **Slope**: Represents the increase in salary for each additional year of experience.\n",
    "\n",
    "### Q4: Concept of Gradient Descent\n",
    "\n",
    "**Definition**: Gradient descent is an optimization algorithm used to minimize the cost function (error) of a model by iteratively adjusting the model parameters. It works by calculating the gradient (derivative) of the cost function with respect to the parameters and updating the parameters in the direction of the negative gradient.\n",
    "\n",
    "**Usage in Machine Learning**:\n",
    "- **Training Models**: Used to find the optimal parameters (weights) for linear regression, neural networks, and other models.\n",
    "- **Process**: \n",
    "  - Initialize parameters.\n",
    "  - Compute the gradient of the cost function.\n",
    "  - Update parameters by moving in the direction of the negative gradient.\n",
    "  - Repeat until convergence.\n",
    "\n",
    "### Q5: Multiple Linear Regression Model\n",
    "\n",
    "**Description**: Multiple linear regression extends simple linear regression by using multiple independent variables to predict a dependent variable. It captures more complex relationships between the predictors and the outcome.\n",
    "\n",
    "**Difference from Simple Linear Regression**:\n",
    "- **Simple Linear Regression**: One predictor variable.\n",
    "- **Multiple Linear Regression**: Multiple predictor variables.\n",
    "\n",
    "### Q6: Multicollinearity in Multiple Linear Regression\n",
    "\n",
    "**Definition**: Multicollinearity occurs when independent variables are highly correlated with each other, causing redundancy and instability in the coefficient estimates.\n",
    "\n",
    "**Detection**:\n",
    "- **Correlation Matrix**: High correlations between predictors.\n",
    "- **Variance Inflation Factor (VIF)**: VIF values above 10 suggest multicollinearity.\n",
    "\n",
    "**Addressing Multicollinearity**:\n",
    "- **Remove Variables**: Exclude highly correlated predictors.\n",
    "- **Combine Variables**: Create composite variables.\n",
    "- **Regularization**: Use techniques like Ridge or Lasso regression to penalize large coefficients.\n",
    "\n",
    "### Q7: Polynomial Regression Model\n",
    "\n",
    "**Definition**: Polynomial regression is an extension of linear regression that models the relationship between the independent and dependent variables as an \\( n \\)-degree polynomial. It allows for non-linear relationships.\n",
    "\n",
    "**Equation**: \\( Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + ... + \\beta_n X^n + \\epsilon \\)\n",
    "\n",
    "**Difference from Linear Regression**:\n",
    "- **Linear Regression**: Models linear relationships.\n",
    "- **Polynomial Regression**: Models non-linear relationships using polynomial terms.\n",
    "\n",
    "### Q8: Advantages and Disadvantages of Polynomial Regression\n",
    "\n",
    "**Advantages**:\n",
    "- **Flexibility**: Can model complex, non-linear relationships.\n",
    "- **Better Fit**: Often provides a better fit to the data when the relationship is non-linear.\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Overfitting**: Higher-degree polynomials can lead to overfitting.\n",
    "- **Complexity**: More complex models can be harder to interpret.\n",
    "\n",
    "**When to Use**:\n",
    "- Use polynomial regression when you suspect a non-linear relationship between the predictor and response variables and when you have enough data to avoid overfitting.\n",
    "\n",
    "**Example Situations**:\n",
    "- Modeling the relationship between age and income, where income might increase at an increasing rate with age.\n",
    "- Predicting the trajectory of a projectile where the relationship between time and height is quadratic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d55cb6-4966-4012-826b-896f757dfdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
