{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d000e27-e1f3-4026-a9a1-19401a8c60e6",
   "metadata": {},
   "source": [
    "### Q1: What is Gradient Boosting Regression?\n",
    "\n",
    "**Gradient Boosting Regression** is an ensemble learning technique that combines the predictions from multiple weak learners (usually decision trees) to improve overall performance in regression tasks. It works by iteratively training models to correct the errors of previous models. Each new model is trained to predict the residuals (errors) of the combined predictions of all previous models.\n",
    "\n",
    "### Q2: Implement a Simple Gradient Boosting Algorithm from Scratch\n",
    "\n",
    "Here’s a basic implementation of gradient boosting for a regression problem using Python and NumPy:\n",
    "\n",
    "**1. Define the Basic Components**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2.5, 3.5, 4.5, 5.5, 6.5])\n",
    "\n",
    "# Define the Gradient Boosting class\n",
    "class SimpleGradientBoosting:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.alphas = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.mean(y) * np.ones_like(y)\n",
    "        for _ in range(self.n_estimators):\n",
    "            residual = y - y_pred\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residual)\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "            self.models.append(model)\n",
    "            self.alphas.append(self.learning_rate)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = np.mean(y) * np.ones(X.shape[0])\n",
    "        for model, alpha in zip(self.models, self.alphas):\n",
    "            y_pred += alpha * model.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Create and train the model\n",
    "gbm = SimpleGradientBoosting(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gbm.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbm.predict(X)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- **`SimpleGradientBoosting`**: This class implements gradient boosting with decision trees as weak learners.\n",
    "- **`fit` Method**: Initializes predictions, computes residuals, trains a new decision tree on residuals, updates predictions, and stores the model.\n",
    "- **`predict` Method**: Aggregates predictions from all trained models to make the final prediction.\n",
    "\n",
    "### Q3: Experiment with Hyperparameters\n",
    "\n",
    "To optimize hyperparameters, you can use grid search or random search. Here’s an example using grid search with `sklearn`:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best parameters and performance\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (Negative MSE):\", grid_search.best_score_)\n",
    "```\n",
    "\n",
    "### Q4: What is a Weak Learner in Gradient Boosting?\n",
    "\n",
    "A **weak learner** is a model that performs slightly better than random guessing. In gradient boosting, weak learners are typically simple models like decision trees with limited depth (e.g., decision stumps). The goal is to combine these weak learners to create a strong, accurate model.\n",
    "\n",
    "### Q5: Intuition Behind the Gradient Boosting Algorithm\n",
    "\n",
    "Gradient Boosting works by:\n",
    "\n",
    "1. **Starting with a Simple Model**: Begin with a simple model that makes basic predictions.\n",
    "2. **Iterative Improvement**: Sequentially add models that focus on the errors (residuals) made by the previous models.\n",
    "3. **Combining Models**: Aggregate the predictions from all models to improve accuracy and reduce errors.\n",
    "\n",
    "This approach iteratively refines the model, making it more accurate by correcting previous mistakes.\n",
    "\n",
    "### Q6: How Gradient Boosting Builds an Ensemble of Weak Learners\n",
    "\n",
    "Gradient Boosting builds an ensemble of weak learners by:\n",
    "\n",
    "1. **Training Sequentially**: Each new weak learner is trained on the residuals of the combined predictions of all previous learners.\n",
    "2. **Updating Predictions**: Predictions are updated by adding the contributions of each new learner.\n",
    "3. **Combining Predictions**: The final prediction is the weighted sum of the predictions from all learners.\n",
    "\n",
    "### Q7: Steps Involved in Constructing the Mathematical Intuition of Gradient Boosting\n",
    "\n",
    "1. **Initialize Model**: Start with a base model that makes initial predictions.\n",
    "2. **Compute Residuals**: Calculate the residuals or errors of the current model.\n",
    "3. **Train New Learner**: Train a new weak learner to predict these residuals.\n",
    "4. **Update Model**: Add the new learner’s predictions to the existing model’s predictions.\n",
    "5. **Iterate**: Repeat steps 2-4 for a specified number of iterations or until improvements are minimal.\n",
    "6. **Combine**: Aggregate the predictions from all learners to make the final prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362c0c7-784e-4ed2-9d65-7d01f5cfff94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
